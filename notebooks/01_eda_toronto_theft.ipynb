{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Toronto Auto Theft Data: Exploratory Data Analysis (EDA)\n",
    "\n",
    "This notebook explores, cleans, and analyzes the Toronto auto theft dataset. We address data quality, perform feature engineering, and extract key insights about temporal and spatial patterns of auto theft in Toronto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "theft_data_path = \"../data/00_raw/auto_theft.csv\"\n",
    "df_theft = pd.read_csv(theft_data_path)\n",
    "print(\"--- Head ---\")\n",
    "display(df_theft.head())\n",
    "print(\"\\n--- Info ---\")\n",
    "df_theft.info()\n",
    "print(\"\\n--- Describe ---\")\n",
    "display(df_theft.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "From this first glance, we can understand a lot about the theft dataset:\n",
    "\n",
    "- **Content & Scope**: The dataset contains 68,063 records of auto theft incidents in Toronto, detailed across 31 columns. The data spans from 2014 to 2024.\n",
    "- **Data Granularity**: We have detailed temporal information (year, month, day, hour) for both the report date and the occurrence date. We also have specific location data, including police division, neighborhood name, and geographic coordinates.\n",
    "- **Key Variables**:\n",
    "  - **Temporal**: `REPORT_DATE`, `OCC_DATE`, and their component columns.\n",
    "  - **Geospatial**: `DIVISION`, `NEIGHBOURHOOD_140`, `NEIGHBOURHOOD_158`, `LONG_WGS84`, `LAT_WGS84`.\n",
    "  - **Categorical**: `OCC_DOW`, `OCC_Month`, `REPORT_DOW`, `REPORT_Month`, `LOCATION_TYPE` (e.g., `Parking Lot`, `Street`), `PREMISES_TYPE` (e.g., `Commercial`, `Residential`).\n",
    "- **Potential Issues**:\n",
    "  - **Missing Data**: There are 4 missing values in the occurrence date columns (`OCC_YEAR`, `OCC_MONTH`, etc.).\n",
    "  - **Incorrect Data Types**:\n",
    "    - `REPORT_DATE` and `OCC_DATE` are object (text) types, not dates. `OCC_YEAR` and `OCC_DAY` are float64 instead of integers.\n",
    "    - For memory efficiency, categorical text and date component columns can be converted to categorical types respectively.\n",
    "  - **Redundancy**:\n",
    "    - `OFFENCE`, `MCI_CATEGORY`, `UCR_CODE`, and `UCR_EXT` columns have only one unique value and can be removed.\n",
    "    - Neighborhood names and IDs `HOOD_158` and `NEIGHBOURHOOD_158` are based on City of Toronto's new 158 neighbourhood structure, while `HOOD_140` and `NEIGHBOURHOOD_140` are based on the old 140 neighbourhood structure. The 158 structure is more recent and granular and should be prioritized.\n",
    "  - **Data Errors**:\n",
    "    - The minimum values for `LAT_WGS84` and `LONG_WGS84` are 0, which is an invalid coordinate for Toronto and indicates data entry errors.\n",
    "    - The null values for categorical variables like `DIVISION`, `HOOD_158`, and `NEIGHBOURHOOD_158` has been specified as \"NSA\" (Not Specified), which is a placeholder and should be treated as missing data.\n",
    "  - **Trailing Whitespace**: `REPORT_DOW` and `OCC_DOW` have trailing whitespace, which needs to be stripped before converting to categorical types.\n",
    "  - **Duplicate Events**: `EVENT_UNIQUE_ID` is not entirely unique, suggesting some events might have multiple entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type and cleaning setup\n",
    "date_columns = [\"REPORT_DATE\", \"OCC_DATE\"]\n",
    "column_dtypes = {\n",
    "    \"EVENT_UNIQUE_ID\": \"object\",\n",
    "    \"REPORT_YEAR\": \"Int16\",\n",
    "    \"REPORT_MONTH\": \"category\",\n",
    "    \"REPORT_DAY\": \"Int16\",\n",
    "    \"REPORT_DOY\": \"Int16\",\n",
    "    \"REPORT_HOUR\": \"Int16\",\n",
    "    \"OCC_YEAR\": \"Int16\",\n",
    "    \"OCC_MONTH\": \"category\",\n",
    "    \"OCC_DAY\": \"Int16\",\n",
    "    \"OCC_DOY\": \"Int16\",\n",
    "    \"OCC_HOUR\": \"Int16\",\n",
    "    \"DIVISION\": \"category\",\n",
    "    \"LOCATION_TYPE\": \"category\",\n",
    "    \"PREMISES_TYPE\": \"category\",\n",
    "    \"HOOD_158\": \"category\",\n",
    "    \"NEIGHBOURHOOD_158\": \"category\",\n",
    "    \"LONG_WGS84\": \"float64\",\n",
    "    \"LAT_WGS84\": \"float64\",\n",
    "}\n",
    "columns_to_drop = [\n",
    "    \"OBJECTID\",\n",
    "    \"OFFENCE\",\n",
    "    \"MCI_CATEGORY\",\n",
    "    \"HOOD_140\",\n",
    "    \"NEIGHBOURHOOD_140\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"UCR_CODE\",\n",
    "    \"UCR_EXT\",\n",
    "]\n",
    "\n",
    "na_values_dict = {\n",
    "    \"LAT_WGS84\": [0, \"0\", \"0.0\"],\n",
    "    \"LONG_WGS84\": [0, \"0\", \"0.0\"],\n",
    "    \"DIVISION\": [\"NSA\"],\n",
    "    \"HOOD_158\": [\"NSA\"],\n",
    "    \"NEIGHBOURHOOD_158\": [\"NSA\"],\n",
    "}\n",
    "converter = {\"REPORT_DOW\": str.strip, \"OCC_DOW\": str.strip}\n",
    "\n",
    "df_theft_clean = pd.read_csv(\n",
    "    theft_data_path,\n",
    "    parse_dates=date_columns,\n",
    "    dtype=column_dtypes,\n",
    "    usecols=lambda col: col not in columns_to_drop,\n",
    "    na_values=na_values_dict,\n",
    "    converters=converter,\n",
    ")\n",
    "df_theft_clean[\"REPORT_DOW\"] = df_theft_clean[\"REPORT_DOW\"].astype(\"category\")\n",
    "df_theft_clean[\"OCC_DOW\"] = df_theft_clean[\"OCC_DOW\"].astype(\"category\")\n",
    "display(df_theft_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Missing Data Visualization\n",
    "Visualize missing data to understand patterns and plan imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df_theft_clean.isnull(), cbar=False, yticklabels=False)\n",
    "plt.title(\"Missing Values Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Data Deduplication and Imputation\n",
    "\n",
    "Let's do some basic checks in order to plan for further cleaning. First,\n",
    "we check if duplicate events are due to multiple entries for the same event or if it is a mistake in creating the ids in `EVENT_UNIQUE_ID`.\n",
    "It seems that timestamps in `REPORT_DATE` and `OCC_DATE` are all 05:00:00, which is likely a placeholder. We need to check if all events have the same timestamp or if there are some variations. Finally with obtain the number of missing values in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop non-unique rows (all columns must match)\n",
    "df_unique_rows = df_theft_clean.drop_duplicates(keep=\"first\")\n",
    "print(f\"\\n--- Number of unique rows: {len(df_unique_rows)} ---\")\n",
    "\n",
    "# check if the number of unique rows is different\n",
    "# from the number of unique EVENT_UNIQUE_ID\n",
    "if len(df_unique_rows) != df_unique_rows[\"EVENT_UNIQUE_ID\"].nunique():\n",
    "    print(\"Warning: EVENT_UNIQUE_ID duplicates differ in other columns.\")\n",
    "else:\n",
    "    print(\"--- All duplicated EVENT_UNIQUE_ID have the same values in all columns ---\")\n",
    "\n",
    "# check if timestamps in REPORT_DATE and OCC_DATE are equal\n",
    "for col in [\"REPORT_DATE\", \"OCC_DATE\"]:\n",
    "    if df_theft_clean[col].dt.time.nunique() == 1:\n",
    "        print(f\"All {col} timestamps are the same.\")\n",
    "\n",
    "# Display the number of null values in each column\n",
    "print(\"\\n--- Number of null values in each column ---\")\n",
    "print(df_theft_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "- There are 61534 unique `EVENT_UNIQUE_ID` values and duplicate events have the same info. So we can drop the duplicates.\n",
    "- It can be seen that the timestamps in `REPORT_DATE` and `OCC_DATE` are all 05:00:00. We will replace these with values inferred from the `REPORT_HOUR` and `OCC_HOUR` columns, which are more accurate.\n",
    "- There are some missing values in geospatial columns, which we need to investigate further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "We analyze the missingness patterns in key geospatial columns of the df_theft_clean DataFrame. This helps understand the extent and nature of missing geospatial information, which is crucial for deciding on imputation strategies or data exclusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_geo = df_theft_clean[\"LAT_WGS84\"].isna() & df_theft_clean[\"LONG_WGS84\"].isna()\n",
    "na_hood = df_theft_clean[\"HOOD_158\"].isna()\n",
    "na_div = df_theft_clean[\"DIVISION\"].isna()\n",
    "\n",
    "print(\"rows lacking LAT/LONG only      :\", (na_geo & ~na_hood & ~na_div).sum())\n",
    "print(\"rows lacking hood but have geo  :\", (~na_geo & na_hood).sum())\n",
    "print(\"rows lacking div  but have geo  :\", (~na_geo & na_div).sum())\n",
    "print(\"rows missing all three          :\", (na_geo & na_hood & na_div).sum())\n",
    "print(\"rows lacking hood but have div  :\", (na_hood & ~na_div).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Based on the results of the previous analysis, the imputation and data cleaning strategy is as follows:\n",
    "\n",
    "- The timestamp in `REPORT_DATE` and `OCC_DATE` is replaced with the values inferred from the `REPORT_HOUR` and `OCC_HOUR` columns.\n",
    "- Duplicate events are dropped, retaining only unique `EVENT_UNIQUE_ID` entries.\n",
    "- Rows missing `OCC_DATE` components (`OCC_YEAR`, `OCC_MONTH`, `OCC_DAY`, `OCC_HOUR`) are dropped (4 rows).\n",
    "- Calendar months names are converted to integer type for faster operations.\n",
    "- Rows missing all three (363 rows, less than 1%): These rows are entirely dropped from the dataset because they lack any geospatial information.\n",
    "\n",
    "- Rows lacking hood but have division (433 rows):\n",
    "  - The missing `HOOD_158` and `NEIGHBOURHOOD_158` for these rows is imputed. The strategy is to use the most frequent `HOOD_158` (the mode) associated with the `DIVISION` present in that row.\n",
    "  - A lookup table (`hood_mode_by_div`) is created by grouping the data (where `HOOD_158` is not null) by `DIVISION` and finding the modal `HOOD_158` for each division.\n",
    "  - This modal `HOOD_158` is then used to fill the missing `HOOD_158` values for rows where `DIVISION` is known.\n",
    "  - Subsequently, the `NEIGHBOURHOOD_158` (the name of the neighborhood) is filled by looking up the imputed `HOOD_158` in a mapping created from non-null `HOOD_158` and `NEIGHBOURHOOD_158` pairs.\n",
    "- Imputing Missing Latitude/Longitude: After the neighborhood information is filled (either originally present or imputed as described above), any remaining missing `LAT_WGS84` and `LONG_WGS84` values are imputed.\n",
    "  - This is done by calculating the median latitude and longitude (centroid) for each `HOOD_158`.\n",
    "  - Rows with missing coordinates are then filled with the centroid coordinates of their respective `HOOD_158`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the hour time in REPORT_DATE and OCC_DATE based on REPORT_HOUR and OCC_HOUR\n",
    "df_theft_clean[\"REPORT_DATE\"] = df_theft_clean.apply(\n",
    "    lambda x: x[\"REPORT_DATE\"].replace(hour=int(x[\"REPORT_HOUR\"]), minute=0, second=0)\n",
    "    if pd.notnull(x[\"REPORT_HOUR\"])\n",
    "    else x[\"REPORT_DATE\"],\n",
    "    axis=1,\n",
    ")\n",
    "df_theft_clean[\"OCC_DATE\"] = df_theft_clean.apply(\n",
    "    lambda x: x[\"OCC_DATE\"].replace(hour=int(x[\"OCC_HOUR\"]), minute=0, second=0)\n",
    "    if pd.notnull(x[\"OCC_HOUR\"])\n",
    "    else x[\"OCC_DATE\"],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# drop duplicated rows again after fixing the time\n",
    "df_theft_clean = df_theft_clean.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# drop rows with OCC_DATE components that are all null\n",
    "df_theft_clean = df_theft_clean.dropna(\n",
    "    subset=[\"OCC_YEAR\", \"OCC_DAY\", \"OCC_DOY\"],\n",
    "    how=\"all\",\n",
    ")\n",
    "\n",
    "# drop rows with no geospatial information\n",
    "mask = (\n",
    "    df_theft_clean[\"LAT_WGS84\"].isna()\n",
    "    & df_theft_clean[\"HOOD_158\"].isna()\n",
    "    & df_theft_clean[\"DIVISION\"].isna()\n",
    ")\n",
    "df_theft_clean = df_theft_clean[~mask].reset_index(drop=True)\n",
    "\n",
    "# impute hoods with the division mode\n",
    "## 1. Build a lookup: DIVISION  ->  modal HOOD_158\n",
    "hood_mode_by_div = (\n",
    "    df_theft_clean[df_theft_clean[\"HOOD_158\"].notna()]  # only rows with a known hood\n",
    "    .groupby(\"DIVISION\")[\"HOOD_158\"]\n",
    "    .agg(lambda s: s.mode().iat[0])  # pick first of modes if tie\n",
    ")\n",
    "\n",
    "## 2. Build a lookup to get the neighbourhood *name* as well\n",
    "name_lookup = (\n",
    "    df_theft_clean[[\"HOOD_158\", \"NEIGHBOURHOOD_158\"]]\n",
    "    .dropna()\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"HOOD_158\")[\"NEIGHBOURHOOD_158\"]\n",
    ")\n",
    "\n",
    "## 3. Apply to rows whose hood is missing but division known\n",
    "mask = df_theft_clean[\"HOOD_158\"].isna() & df_theft_clean[\"DIVISION\"].notna()\n",
    "\n",
    "df_theft_clean.loc[mask, \"HOOD_158\"] = df_theft_clean.loc[mask, \"DIVISION\"].map(\n",
    "    hood_mode_by_div\n",
    ")\n",
    "df_theft_clean.loc[mask, \"NEIGHBOURHOOD_158\"] = df_theft_clean.loc[\n",
    "    mask, \"HOOD_158\"\n",
    "].map(name_lookup)\n",
    "\n",
    "\n",
    "# Impute missing values in LONG_WGS84 and LAT_WGS84\n",
    "# calculate centroids for each neighbourhood\n",
    "centroids = df_theft_clean.groupby(\"HOOD_158\", observed=False)[\n",
    "    [\"LAT_WGS84\", \"LONG_WGS84\"]\n",
    "].median()\n",
    "\n",
    "# fillna with the centroid of the same neighbourhood\n",
    "df_theft_clean[[\"LAT_WGS84\", \"LONG_WGS84\"]] = (\n",
    "    df_theft_clean.set_index(\"HOOD_158\")\n",
    "    .join(centroids, rsuffix=\"_cent\")\n",
    "    .assign(\n",
    "        LAT_WGS84=lambda x: x[\"LAT_WGS84\"].fillna(x[\"LAT_WGS84_cent\"]),\n",
    "        LONG_WGS84=lambda x: x[\"LONG_WGS84\"].fillna(x[\"LONG_WGS84_cent\"]),\n",
    "    )\n",
    "    .drop(columns=[\"LAT_WGS84_cent\", \"LONG_WGS84_cent\"])\n",
    "    .reset_index()[[\"LAT_WGS84\", \"LONG_WGS84\"]]\n",
    ")\n",
    "\n",
    "df_theft_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Data Sanity and Consistency Checks\n",
    "\n",
    "This section performs several checks to ensure the integrity and consistency of the cleaned dataset (`df_theft_clean`).\n",
    "\n",
    "1.  **Geographic Coordinate Validation**:\n",
    "    It verifies that all latitude (`LAT_WGS84`) and longitude (`LONG_WGS84`) coordinates fall within the approximate boundaries of Toronto (latitude between 43.5 and 44.0, longitude between -79.8 and -79.0).\n",
    "\n",
    "1.  **Occurrence and Report Date Logic**:\n",
    "    It checks if the occurrence date (`OCC_DATE`) is always earlier than or equal to the report date (`REPORT_DATE`).\n",
    "\n",
    "1.  **Date Component Verification (Occurrence Date)**:\n",
    "    It compares the individual date components (`OCC_YEAR`, `OCC_MONTH`, `OCC_DAY`, `OCC_DOY`, `OCC_DOW`) against the corresponding parts extracted from the `OCC_DATE` timestamp.\n",
    "\n",
    "1.  **Date Component Verification (Report Date)**:\n",
    "    It compares the individual date components (`REPORT_YEAR`, `REPORT_MONTH`, `REPORT_DAY`, `REPORT_DOY`, `REPORT_DOW`) against the corresponding parts extracted from the `REPORT_DATE` timestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the coordinates are within the expected range\n",
    "mask = (\n",
    "    (df_theft_clean[\"LAT_WGS84\"] >= 43.5)\n",
    "    & (df_theft_clean[\"LAT_WGS84\"] <= 44.0)\n",
    "    & (df_theft_clean[\"LONG_WGS84\"] >= -79.8)\n",
    "    & (df_theft_clean[\"LONG_WGS84\"] <= -79.0)\n",
    ")\n",
    "if not mask.all():\n",
    "    print(\"Warning: Some coordinates are outside the expected range.\")\n",
    "    print(df_theft_clean.loc[~mask, [\"LAT_WGS84\", \"LONG_WGS84\"]].head())\n",
    "else:\n",
    "    print(\"All coordinates are within the expected range.\")\n",
    "\n",
    "# check if OCC_DATE <= REPORT_DATE\n",
    "mask = df_theft_clean[\"OCC_DATE\"] <= df_theft_clean[\"REPORT_DATE\"]\n",
    "if not mask.all():\n",
    "    print(\"Warning: Some OCC_DATE are later than REPORT_DATE.\")\n",
    "    print(df_theft_clean.loc[~mask, [\"OCC_DATE\", \"REPORT_DATE\"]].head())\n",
    "else:\n",
    "    print(\"All OCC_DATE are earlier than or equal to REPORT_DATE.\")\n",
    "\n",
    "# check if values of date columns are between 2014 and 2024\n",
    "valid_years = range(2013, 2025)  # 2014 to 2024 inclusive\n",
    "mask = df_theft_clean[\"OCC_YEAR\"].isin(valid_years) & df_theft_clean[\n",
    "    \"REPORT_YEAR\"\n",
    "].isin(valid_years)\n",
    "if not mask.all():\n",
    "    num_invalid = (~mask).sum()\n",
    "    print(f\"Warning: {num_invalid} dates are outside the valid range (2014-2024).\")\n",
    "    print(\n",
    "        df_theft_clean.loc[~mask, [\"OCC_YEAR\", \"REPORT_YEAR\", \"REPORT_DATE\"]].head(100)\n",
    "    )\n",
    "else:\n",
    "    print(\"All dates are within the valid range (2014-2024).\")\n",
    "\n",
    "# check if Year/month/day columns match the date columns\n",
    "date_component_map = {\n",
    "    \"YEAR\": \"year\",\n",
    "    \"MONTH\": \"month_name\",\n",
    "    \"DAY\": \"day\",\n",
    "    \"DOY\": \"dayofyear\",\n",
    "    \"DOW\": \"day_name\",\n",
    "}\n",
    "\n",
    "for prefix in [\"OCC\", \"REPORT\"]:\n",
    "    date_col_name = f\"{prefix}_DATE\"  # e.g., \"OCC_DATE\"\n",
    "    for comp_suffix, dt_attr in date_component_map.items():\n",
    "        comp_col_name = f\"{prefix}_{comp_suffix}\"  # e.g., \"OCC_YEAR\"\n",
    "\n",
    "        if comp_suffix not in [\"DOW\", \"MONTH\"]:\n",
    "            # Extract values from the main date column (e.g., OCC_DATE.dt.year)\n",
    "            date_derived_values = getattr(\n",
    "                df_theft_clean[date_col_name].dt, dt_attr\n",
    "            ).astype(\"Int16\")\n",
    "            component_col_values = pd.to_numeric(\n",
    "                df_theft_clean[comp_col_name], errors=\"coerce\"\n",
    "            ).astype(\"Int16\")\n",
    "        else:\n",
    "            # For DOW and MONTH, we use the category dtype directly\n",
    "            date_derived_values = getattr(df_theft_clean[date_col_name].dt, dt_attr)()\n",
    "            component_col_values = df_theft_clean[comp_col_name].astype(\"object\")\n",
    "\n",
    "        if not date_derived_values.equals(component_col_values):\n",
    "            print(f\"Warning: {comp_col_name} does not match {date_col_name}.\")\n",
    "            mismatched_mask = (date_derived_values != component_col_values) & (\n",
    "                date_derived_values.notna() | component_col_values.notna()\n",
    "            )\n",
    "            if mismatched_mask.any():\n",
    "                print(f\"Mismatched entries for {comp_col_name}:\")\n",
    "                print(\n",
    "                    df_theft_clean.loc[\n",
    "                        mismatched_mask, [date_col_name, comp_col_name]\n",
    "                    ].head()\n",
    "                )\n",
    "        else:\n",
    "            print(f\"{comp_col_name} matches {date_col_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "There are 19 events with `OCC_DATE`s that are significantly earlier than the reporting period which this dataset covers, which is considered as an anomaly. While it is possible that some reports are filed long after the actual occurrence, this is not common in auto theft cases and we drop these rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with OCC_DATE before 2013\n",
    "df_theft_clean = df_theft_clean[df_theft_clean[\"OCC_DATE\"] >= \"2013-01-01\"].reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This section focuses on creating new features from existing columns to enhance the dataset for analysis and modeling. The new features are:\n",
    "\n",
    "1.  **`OCC_TIME_BIN`**: Categorizes the `OCC_HOUR` into discrete time bins:\n",
    "\n",
    "    - **Night**: 00:00 - 05:59 and 22:00 - 23:59\n",
    "    - **Morning**: 06:00 - 11:59\n",
    "    - **Afternoon**: 12:00 - 17:59\n",
    "    - **Evening**: 18:00 - 21:59\n",
    "\n",
    "2.  **`SEASON`**: Derives the season from the `OCC_MONTH`:\n",
    "\n",
    "    - **Winter**: December, January, February\n",
    "    - **Spring**: March, April, May\n",
    "    - **Summer**: June, July, August\n",
    "    - **Autumn**: September, October, November\n",
    "\n",
    "3.  **`IS_WEEKEND`**: A boolean feature indicating whether the `OCC_DOW` falls on a weekend (Saturday or Sunday).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCC_TIME_BIN\n",
    "hour_bins = [-1, 5, 11, 17, 21, 23]  # Bins: Night, Morning, Afternoon, Evening, Night\n",
    "hour_labels = [\"Night\", \"Morning\", \"Afternoon\", \"Evening\", \"Night\"]\n",
    "df_theft_clean[\"OCC_TIME_BIN\"] = pd.cut(\n",
    "    df_theft_clean[\"OCC_HOUR\"], bins=hour_bins, labels=hour_labels, ordered=False\n",
    ")\n",
    "\n",
    "# SEASON\n",
    "# Month to season mapping\n",
    "season_map = {\n",
    "    \"January\": \"Winter\",\n",
    "    \"February\": \"Winter\",\n",
    "    \"March\": \"Spring\",\n",
    "    \"April\": \"Spring\",\n",
    "    \"May\": \"Spring\",\n",
    "    \"June\": \"Summer\",\n",
    "    \"July\": \"Summer\",\n",
    "    \"August\": \"Summer\",\n",
    "    \"September\": \"Autumn\",\n",
    "    \"October\": \"Autumn\",\n",
    "    \"November\": \"Autumn\",\n",
    "    \"December\": \"Winter\",\n",
    "}\n",
    "df_theft_clean[\"SEASON\"] = df_theft_clean[\"OCC_MONTH\"].map(season_map)\n",
    "df_theft_clean[\"SEASON\"] = pd.Categorical(\n",
    "    df_theft_clean[\"SEASON\"],\n",
    "    categories=[\"Winter\", \"Spring\", \"Summer\", \"Autumn\"],\n",
    "    ordered=False,\n",
    ")\n",
    "\n",
    "\n",
    "# IS_WEEKEND\n",
    "# OCC_DOW\n",
    "df_theft_clean[\"IS_WEEKEND\"] = df_theft_clean[\"OCC_DOW\"].isin([\"Saturday\", \"Sunday\"])\n",
    "\n",
    "df_theft_clean[\n",
    "    [\"OCC_HOUR\", \"OCC_TIME_BIN\", \"OCC_MONTH\", \"SEASON\", \"OCC_DOW\", \"IS_WEEKEND\"]\n",
    "].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- df_theft_clean.info() ---\")\n",
    "df_theft_clean.info()\n",
    "print(\"\\n--- df_theft_clean.describe() ---\")\n",
    "display(df_theft_clean.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts for OCC_TIME_BIN\n",
    "time_bin_counts = df_theft_clean[\"OCC_TIME_BIN\"].value_counts()\n",
    "\n",
    "# Create the bar plot using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x=time_bin_counts.index,\n",
    "    y=time_bin_counts.values,\n",
    "    palette=\"viridis\",\n",
    "    hue=time_bin_counts.index,\n",
    "    legend=False,\n",
    ")\n",
    "plt.title(\"Occurrences by Time of Day\")\n",
    "plt.xlabel(\"Time of Day\")\n",
    "plt.ylabel(\"Number of Occurrences\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts for OCC_DOW\n",
    "dow_counts = df_theft_clean[\"OCC_DOW\"].value_counts()\n",
    "\n",
    "dow_counts = dow_counts.reindex(\n",
    "    [\n",
    "        \"Monday\",\n",
    "        \"Tuesday\",\n",
    "        \"Wednesday\",\n",
    "        \"Thursday\",\n",
    "        \"Friday\",\n",
    "        \"Saturday\",\n",
    "        \"Sunday\",\n",
    "    ],\n",
    "    fill_value=0,\n",
    ")\n",
    "\n",
    "# Create the bar plot using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x=dow_counts.index,\n",
    "    y=dow_counts.values,\n",
    "    palette=\"viridis\",\n",
    "    hue=dow_counts.index,\n",
    "    legend=False,\n",
    ")\n",
    "plt.title(\"Occurrences by Day of Week\")\n",
    "plt.xlabel(\"Day of Week\")\n",
    "plt.ylabel(\"Number of Occurrences\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts for SEASON\n",
    "season_counts = df_theft_clean[\"SEASON\"].value_counts()\n",
    "\n",
    "# calculate the percentage of occurrences in each season\n",
    "season_percentages = season_counts / season_counts.sum() * 100\n",
    "\n",
    "display(season_percentages)\n",
    "\n",
    "# Create the bar plot using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x=season_counts.index,\n",
    "    y=season_counts.values,\n",
    "    palette=\"viridis\",\n",
    "    hue=season_counts.index,\n",
    "    legend=False,\n",
    ")\n",
    "plt.title(\"Occurrences by Season\")\n",
    "plt.xlabel(\"Season\")\n",
    "plt.ylabel(\"Number of Occurrences\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts for DIVISION\n",
    "division_counts = df_theft_clean[\"DIVISION\"].value_counts()\n",
    "\n",
    "# Create the bar plot using seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x=division_counts.index,\n",
    "    y=division_counts.values,\n",
    "    palette=\"viridis\",\n",
    "    hue=division_counts.index,\n",
    "    legend=False,\n",
    ")\n",
    "plt.title(\"Occurrences by Police Division\")\n",
    "plt.xlabel(\"Police Division\")\n",
    "plt.ylabel(\"Number of Occurrences\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts for PREMISES_TYPE\n",
    "premises_counts = df_theft_clean[\"PREMISES_TYPE\"].value_counts()\n",
    "\n",
    "# Create the bar plot using seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x=premises_counts.index,\n",
    "    y=premises_counts.values,\n",
    "    palette=\"viridis\",\n",
    "    hue=premises_counts.index,\n",
    "    legend=False,\n",
    ")\n",
    "plt.title(\"Occurrences by Premises Type\")\n",
    "plt.xlabel(\"Premises Type\")\n",
    "plt.ylabel(\"Number of Occurrences\")\n",
    "plt.xticks(rotation=45, ha=\"right\") # Rotate labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Key Insights from Cleaned Auto Theft Data\n",
    "\n",
    "The initial cleaning and feature engineering steps have yielded several important insights into the Toronto auto theft dataset:\n",
    "\n",
    "**Data Quality & Integrity:**\n",
    "\n",
    "- **Successful Cleaning**: The dataset now comprises 61,196 unique auto theft events, with almost all missing values addressed.\n",
    "- **Optimized Data Types**: Appropriate data types (`datetime64[ns]`, `category`, `Int16`, `boolean`) have been applied, significantly reducing memory usage from over 16MB (the initial ingestion) to approximately 4.6MB.\n",
    "- **Unique Event IDs**: All `EVENT_UNIQUE_ID` entries are now unique, ensuring each row represents a distinct theft incident.\n",
    "\n",
    "**Temporal Patterns:**\n",
    "\n",
    "- **Time of Day for Occurrence**:\n",
    "  - Thefts are most frequently reported as occurring during the **Night** (23,021 incidents).\n",
    "- **Seasonal Trends**:\n",
    "  - Thefts are relatively evenly distributed throughout the year, with a slight peak in **Autumn** (16,498 incidents).\n",
    "- **Weekly Patterns**:\n",
    "  - Thefts are relatively evenly distributed throughout the week. However majority of thefts occur on **Thursday** (9,429 incidents) and weekends has the lowest number of thefts.\n",
    "- **Reporting Lag**: The `REPORT_DATE` often differs from the `OCC_DATE`, indicating a delay between the theft and its reporting. The average report date is around October 23rd, while the average occurrence date is around October 18th for the dataset's timespan.\n",
    "\n",
    "**Geospatial & Location-Based Patterns:**\n",
    "\n",
    "- **Top Hotspot (Neighbourhood)**:\n",
    "  - **West Humber-Clairville (HOOD_158: 001)** consistently emerges as the neighbourhood with the highest number of auto thefts (4,781 incidents).\n",
    "- **Top Hotspot (Police Division)**:\n",
    "  - **Division D23** reports the highest number of auto thefts (8,466 incidents), aligning with the West Humber-Clairville hotspot.\n",
    "- **Common Theft Locations**:\n",
    "  - **Parking Lots (Apartment, Commercial, or Non-Commercial)** are the most common `LOCATION_TYPE` for auto thefts (21,308 incidents).\n",
    "- **Common Premises Type**:\n",
    "  - The most frequent `PREMISES_TYPE` is **Outside** (33,024 incidents), which includes vehicles stolen from streets and unenclosed areas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
