{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Canada Census 2021 Data: Exploratory Data Analysis (EDA)\n",
    "\n",
    "This notebook explores, cleans, and analyzes the Canada census 2021 dataset. We address data quality, filtering, and extract key insights about the demographic and socio-economic characteristics of Toronto's population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Because the census dataset is very large, we want to load only the parts that are relevant to our analysisâ€”specifically, the data related to Toronto. The `census_2021_geo` file contains the geographic names (`Geo Name`) and their corresponding line numbers in the main census dataset. By using these line numbers, we can efficiently read only the rows from the main census file that correspond to Toronto regions, significantly reducing memory usage and processing time. This targeted loading approach allows us to focus our analysis on the Forward Sortation Areas (FSAs) within Toronto without handling the entire national dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_geo_path = \"../data/00_raw/census_2021_geo.csv\"\n",
    "df_geo = pd.read_csv(census_geo_path)\n",
    "\n",
    "display(df_geo.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "The Geo Name values related to Toronto are started with \"M\" (for example, \"M1A\", \"M2B\", etc.). We will filter the `census_2021_geo` file to extract only those rows that correspond to Toronto's FSAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the start and end line numbers for Toronto FSAs (Geo Name starts with \"M\")\n",
    "start_line = df_geo[df_geo[\"Geo Name\"].str.startswith(\"M\")].iloc[0][\"Line Number\"]\n",
    "end_line = df_geo[df_geo[\"Geo Name\"].str.startswith(\"N\")].iloc[0][\"Line Number\"]\n",
    "\n",
    "print(f\"Start line for Toronto FSAs: {start_line}\")\n",
    "print(f\"End line for Toronto FSAs: {end_line}\")\n",
    "\n",
    "# Calculate skiprows and nrows for efficient CSV loading\n",
    "nskiprows = start_line - 1  # skip header and lines before Toronto\n",
    "nrows = end_line - start_line  # number of Toronto rows\n",
    "\n",
    "print(f\"nskiprows: {nskiprows}\")\n",
    "print(f\"nrows: {nrows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_data_path = \"../data/00_raw/census_2021.csv\"\n",
    "df_census = pd.read_csv(\n",
    "    census_data_path,\n",
    "    header=0,\n",
    "    encoding=\"latin1\",\n",
    "    skiprows=range(1, nskiprows),\n",
    "    nrows=nrows,\n",
    ")\n",
    "print(\"--- Head ---\")\n",
    "display(df_census)\n",
    "print(\"\\n--- Info ---\")\n",
    "df_census.info()\n",
    "print(\"\\n--- Describe ---\")\n",
    "display(df_census.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if ALT_GEO_CODE and GEO_NAME are the same\n",
    "df_census[\"ALT_GEO_CODE\"] = df_census[\"ALT_GEO_CODE\"].astype(str)\n",
    "df_census[\"GEO_NAME\"] = df_census[\"GEO_NAME\"].astype(str)\n",
    "print(\"\\n--- Check ALT_GEO_CODE and GEO_NAME ---\")\n",
    "df_census[\"check\"] = df_census[\"ALT_GEO_CODE\"] == df_census[\"GEO_NAME\"]\n",
    "if df_census[\"check\"].all():\n",
    "    print(\"All ALT_GEO_CODE and GEO_NAME match.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### Data Cleaning and Preparation Strategy\n",
    "\n",
    "The initial exploration of the `df_census` DataFrame reveals several areas that need attention before it can be effectively used for analysis and merging with other datasets. The primary goals of this cleaning process are to:\n",
    "1.  Ensure correct and meaningful column names.\n",
    "2.  Optimize data types for memory efficiency and appropriate representation.\n",
    "3.  Understand and prepare for handling missing values, which often have specific meanings in this dataset.\n",
    "4.  Prepare the data for potential pivoting operations.\n",
    "\n",
    "Here's a breakdown of the planned steps:\n",
    "\n",
    "1.  **Address Column Naming (SYMBOL columns):**\n",
    "    *   **Issue:** The dataset contains columns like `SYMBOL`, `SYMBOL.1`, `SYMBOL.2`, etc. These arise because the original CSV has multiple columns named \"SYMBOL\", and pandas automatically renames duplicates upon import.\n",
    "    *   **Solution:** These `SYMBOL` columns provide crucial context (e.g., data suppression, reliability, applicability) for their corresponding data columns (e.g., `C1_COUNT_TOTAL`, `C2_COUNT_MEN+`). They need to be renamed to establish an explicit link. For example:\n",
    "        *   The first `SYMBOL` column (original index 12, next to `C1_COUNT_TOTAL`) will be renamed to `C1_SYMBOL`.\n",
    "        *   `SYMBOL.1` (original index 14, next to `C2_COUNT_MEN+`) will be renamed to `C2_SYMBOL`.\n",
    "        *   This pattern will be applied systematically for all `SYMBOL.n` columns, linking them to `C3_...`, `C4_...` up to `C18_...`.\n",
    "    *   **Benefit:** This renaming will make it significantly easier to interpret data quality and understand missingness for each specific metric.\n",
    "\n",
    "2.  **Optimize Data Types:**\n",
    "    *   **Goal:** Improve memory usage and ensure data is represented in the most appropriate format for analysis.\n",
    "    *   **Categorical Columns:**\n",
    "        *   `DGUID`: While `object` is acceptable, given its role as a geographic identifier that will repeat for different characteristics, converting to `category` can be beneficial if memory becomes a concern. For joining purposes, it should be treated as a string.\n",
    "        *   `ALT_GEO_CODE`: This is the primary key for joining with other datasets (like auto theft data). It should be cleaned (e.g., trim whitespace, ensure consistent casing) and then converted to `category` or kept as `object` (string).\n",
    "        *   `CHARACTERISTIC_ID`: This identifier for census characteristics should be `category`.\n",
    "        *   `CHARACTERISTIC_NAME`: Contains many repeated textual descriptions of census variables. Converting to `category` is highly recommended for memory savings and performance.\n",
    "        *   `DATA_QUALITY_FLAG`: Currently `int64`. If these are discrete codes (as suggested by the `describe` output showing a limited range and integer-like values), converting to `category` is appropriate after verifying unique values.\n",
    "        *   All `SYMBOL` columns (after renaming): These contain codes like 'x', '..', 'F'. They should be converted to `category`.\n",
    "    *   **Numeric Columns:**\n",
    "        *   `TNR_SF`, `TNR_LF`: Currently `float64`. These likely represent \"Total Number of Records\" for short and long forms. If they are always whole numbers, they should be converted to a nullable integer type like `pd.Int16Dtype()` or `pd.Int32Dtype()` after handling NaNs, to save memory and accurately reflect their nature as counts.\n",
    "        *   `CHARACTERISTIC_NOTE`: Currently `float64` with many NaNs. These are likely numerical IDs for footnotes. Convert to a nullable integer type (e.g., `pd.Int16Dtype()`) to handle NaNs appropriately.\n",
    "        *   Data Columns (`C1_COUNT_TOTAL` to `C18_RATE_HI_CI_WOMEN+`): `float64` is generally suitable for these columns as they can represent counts, rates, and confidence intervals, which may include decimal values and will contain NaNs where data is suppressed or not applicable. However, `float32` could be considered if precision allows, to save memory.\n",
    "    *   **Columns to Drop:**\n",
    "        *   `CENSUS_YEAR`: Appears to be constant (2021). Can be dropped if this information is stored elsewhere or considered implicit.\n",
    "        *   `GEO_LEVEL`: Appears to be constant (\"Forward sortation area\"). Can be dropped.\n",
    "        *   `GEO_NAME`: Appears to be identical to `ALT_GEO_CODE` in the sample. If this holds true for the entire dataset, one of them can be dropped to avoid redundancy. `ALT_GEO_CODE` is preferred as it's a more standard geographic code.\n",
    "\n",
    "3.  **Hierarchical Characteristic Names:**\n",
    "    *   **Observation:** The `CHARACTERISTIC_NAME` column has leading spaces, which seem to indicate a hierarchical structure (e.g., \"Total...\", \"  Male...\", \"    Aged 15-24...\").\n",
    "    *   **Action:**\n",
    "        *   Create a new column, `CHARACTERISTIC_LEVEL`, by counting the leading spaces (divided by 2, assuming 2 spaces per indent level) to quantify the hierarchy.\n",
    "        *   Strip leading/trailing whitespace from `CHARACTERISTIC_NAME` itself for cleaner values.\n",
    "\n",
    "By implementing these cleaning steps, the dataset will be more robust, memory-efficient, and easier to work with for subsequent analysis and merging tasks. The explicit linking of `SYMBOL` columns to their data columns is particularly crucial for correctly interpreting the data's meaning and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "column_dtypes = {\n",
    "    \"DGUID\": \"category\",\n",
    "    \"ALT_GEO_CODE\": \"object\",\n",
    "    \"CHARACTERISTIC_ID\": \"category\",\n",
    "    \"CHARACTERISTIC_NAME\": \"object\",\n",
    "    \"CHARACTERISTIC_NOTE\": \"Int16\",\n",
    "    \"DATA_QUALITY_FLAG\": \"category\",\n",
    "    \"TNR_SF\": \"float32\",\n",
    "    \"TNR_LF\": \"float32\",\n",
    "}\n",
    "\n",
    "\n",
    "original_columns = pd.read_csv(census_data_path, nrows=0, encoding=\"latin1\").columns.tolist()\n",
    "new_columns = []\n",
    "for col in original_columns:\n",
    "    if col == \"SYMBOL\":\n",
    "        new_columns.append(\"C1_SYMBOL\")\n",
    "        column_dtypes[\"C1_SYMBOL\"] = \"category\"\n",
    "    elif col.startswith(\"SYMBOL.\"):\n",
    "        idx = int(col.split(\".\")[1])\n",
    "        new_columns.append(f\"C{idx+1}_SYMBOL\")\n",
    "        column_dtypes[f\"C{idx+1}_SYMBOL\"] = \"category\"\n",
    "    else:\n",
    "        new_columns.append(col)\n",
    "\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"CENSUS_YEAR\",\n",
    "    \"GEO_LEVEL\",\n",
    "    \"GEO_NAME\",\n",
    "]\n",
    "\n",
    "df_census = pd.read_csv(\n",
    "    census_data_path,\n",
    "    header=0,\n",
    "    encoding=\"latin1\",\n",
    "    skiprows=range(1, 1450232 + 1),\n",
    "    nrows=157526,\n",
    "    names=new_columns,\n",
    "    usecols=lambda x: x not in columns_to_drop,\n",
    "    dtype=column_dtypes,\n",
    ")\n",
    "\n",
    "# \n",
    "df_census['CHARACTERISTIC_LEVEL'] = df_census['CHARACTERISTIC_NAME'].apply(lambda x: int((len(x) - len(x.lstrip(' ')))/2))\n",
    "df_census['CHARACTERISTIC_NAME'] = df_census['CHARACTERISTIC_NAME'].str.strip().astype(\"category\")\n",
    "\n",
    "\n",
    "# Display the first few rows with new column names and info to verify\n",
    "print(\"--- Head --\")\n",
    "display(df_census.head())\n",
    "print(\"\\n--- Info ---\")\n",
    "df_census.info()\n",
    "print(\"\\n--- Describe ---\")\n",
    "display(df_census.describe(include=\"all\").T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df_census.isnull(), cbar=False, yticklabels=False)\n",
    "plt.title(\"Missing Values Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Understanding and Handling Missing Values in the Census Dataset\n",
    "\n",
    "Missing values in this dataset are not always simply \"missing\"; They often carry specific meanings, especially as indicated by the various `SYMBOL` columns.\n",
    "\n",
    "- `CHARACTERISTIC_NOTE`\n",
    "  - Many NaNs in `CHARACTERISTIC_NOTE` are expected; not every characteristic has a footnote.\n",
    "  - NaN here usually means \"no note\" and can generally be left as is.\n",
    "\n",
    "#### Data Columns (`C1` to `C18`) and Their SYMBOL Columns\n",
    "\n",
    "- **Link Between Data and SYMBOL Columns:**  \n",
    "    A NaN in a data column (e.g., `C1_COUNT_TOTAL`) often corresponds to a value in its paired SYMBOL column (e.g., `C1_SYMBOL` might be `'x'`, `'F'`, `'...'`, or `'..'`).\n",
    "\n",
    "    - `x`: Suppressed for confidentiality.\n",
    "    - `F`: Too unreliable to be published.\n",
    "    - `...`: Not applicable.\n",
    "    - `..`: Not available.\n",
    "\n",
    "- **SYMBOL Column NaNs:**  \n",
    "    If a SYMBOL column is NaN, it typically means the corresponding data value is present and valid (not suppressed or inapplicable).\n",
    "- For example, if `C1_COUNT_TOTAL` is NaN and `C1_SYMBOL` is `'x'`, the data was suppressed. Replacing with 0 would be incorrect.\n",
    "\n",
    "#### Confidence Interval (CI) Columns\n",
    "\n",
    "- CI columns often have more NaNs than main estimates. This is normal, as CIs may not be provided for small or unreliable estimates.\n",
    "\n",
    "#### Strategy After Pivoting\n",
    "\n",
    "- **Suppressed or Unreliable Data (`x`, `F`):**  \n",
    "    These are intentionally missing. We may keep them as NaN and note their status, or retain the symbol if our analysis/model can handle categorical values. Imputation is generally not recommended.\n",
    "- **Not Applicable (`...`):**  \n",
    "    For count data, this might sometimes be replaced with 0, but for rates, interpretation is more complex.\n",
    "- **Not Available (`..`):**  \n",
    "    These are truly missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "df_census['CHARACTERISTIC_NAME'][df_census['CHARACTERISTIC_LEVEL'] == 1].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Depending on the detail and complexity of the analysis, we may choose to filter the dataset based on CHARACTERISTIC_LEVEL. for example CHARACTERISTIC_LEVEL larger than 4 are typically more granular and may not be necessary for high-level analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_census = df_census['CHARACTERISTIC_NAME'][df_census['CHARACTERISTIC_LEVEL'] < 5]\n",
    "\n",
    "print(\"\\n--- Filtered CHARACTERISTIC_NAME ---\")\n",
    "display(df_census)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
