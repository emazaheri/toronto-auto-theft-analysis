{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set styling\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Joint Analysis of Toronto Auto Theft and Census Data\n",
    "\n",
    "This notebook explores the relationship between auto theft incidents and socioeconomic factors in Toronto neighborhoods. The analysis integrates two distinct datasets:\n",
    "\n",
    "1. **Auto Theft Dataset**: Contains detailed records of auto theft incidents across Toronto's 158 neighborhoods, including temporal and location information.\n",
    "\n",
    "2. **Census 2021 Dataset**: Contains demographic and socioeconomic data for Toronto, organized by Forward Sortation Areas (FSAs).\n",
    "\n",
    "## Analytical Challenge\n",
    "\n",
    "The key challenge in this analysis is that the two datasets use different geographic units:\n",
    "\n",
    "- Auto theft data is organized by Toronto's 158 official neighborhoods\n",
    "- Census data is organized by Forward Sortation Areas (FSAs)\n",
    "\n",
    "To address this, we'll use areal-weighted interpolation through the `overlap_df` mapping, which quantifies the percentage overlap between each neighborhood and FSA. This allows us to distribute census data from FSAs to neighborhoods proportionally based on spatial overlap.\n",
    "\n",
    "## Analysis Goals\n",
    "\n",
    "1. Join the auto theft and census datasets using the spatial relationship mapping\n",
    "2. Analyze patterns between socioeconomic factors and auto theft rates\n",
    "3. Identify key insights about which factors may be associated with higher/lower theft rates\n",
    "4. Visualize the spatial distribution of findings across Toronto\n",
    "\n",
    "This approach enables us to uncover potential relationships between community characteristics and auto theft incidents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "overlap_df = pd.read_parquet(\"../data/01_processed/toronto_hoods_fsa_overlap.parquet\")\n",
    "census_df = pd.read_parquet(\"../data/01_processed/census_2021_processed.parquet\")\n",
    "theft_df = pd.read_parquet(\"../data/01_processed/auto_theft_processed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(overlap_df.head())\n",
    "display(census_df.head())\n",
    "display(theft_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the structure of our key datasets\n",
    "# First, let's look at the overlap_df which is crucial for joining the census and theft datasets\n",
    "print(\"\\nOverlap DataFrame Structure:\")\n",
    "print(f\"Shape: {overlap_df.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "for col in overlap_df.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "print(\"\\nFirst few rows of overlap_df:\")\n",
    "display(overlap_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's examine the census dataset\n",
    "print(\"\\nCensus DataFrame Structure:\")\n",
    "print(f\"Shape: {census_df.shape}\")\n",
    "print(\"\\nSelected Columns (first 10):\")\n",
    "for col in census_df.columns[:10]:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "print(\"\\nExample record:\")\n",
    "display(census_df.iloc[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's examine the auto theft dataset\n",
    "print(\"\\nAuto Theft DataFrame Structure:\")\n",
    "print(f\"Shape: {theft_df.shape}\")\n",
    "print(\"\\nSelected Columns (first 10):\")\n",
    "for col in theft_df.columns[:10]:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "print(\"\\nColumns related to location/neighborhood:\")\n",
    "for col in theft_df.columns:\n",
    "    if \"AREA\" in col or \"NEIGHBOURHOOD\" in col or \"DIVISION\" in col:\n",
    "        print(f\"- {col}\")\n",
    "\n",
    "print(\"\\nExample record:\")\n",
    "display(theft_df.iloc[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Step 1: Aggregate Auto Theft Data by Neighborhood\n",
    "\n",
    "First, we need to aggregate the auto theft data by neighborhood to understand the frequency of incidents across Toronto's geography. We'll create metrics such as:\n",
    "\n",
    "- Total number of thefts by neighborhood\n",
    "- Theft rate per year (to account for different time periods)\n",
    "- Seasonal patterns of theft in each neighborhood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count auto thefts by neighborhood\n",
    "theft_by_hood = theft_df.groupby(\"HOOD_158\").size().reset_index(name=\"total_thefts\")\n",
    "\n",
    "# Count thefts by neighborhood and year to calculate averages\n",
    "theft_by_hood_year = (\n",
    "    theft_df.groupby([\"HOOD_158\", \"OCC_YEAR\"]).size().reset_index(name=\"yearly_thefts\")\n",
    ")\n",
    "\n",
    "# Calculate average yearly thefts per neighborhood\n",
    "avg_yearly_thefts = (\n",
    "    theft_by_hood_year.groupby(\"HOOD_158\")[\"yearly_thefts\"]\n",
    "    .mean()\n",
    "    .reset_index(name=\"avg_yearly_thefts\")\n",
    ")\n",
    "\n",
    "# Merge total and average yearly thefts\n",
    "theft_by_hood = theft_by_hood.merge(avg_yearly_thefts, on=\"HOOD_158\")\n",
    "\n",
    "# Let's also examine seasonal patterns\n",
    "seasonal_thefts = (\n",
    "    theft_df.groupby([\"HOOD_158\", \"SEASON\"]).size().reset_index(name=\"seasonal_thefts\")\n",
    ")\n",
    "\n",
    "# Calculate the percentage of thefts by season for each neighborhood\n",
    "seasonal_pct = seasonal_thefts.merge(\n",
    "    theft_by_hood[[\"HOOD_158\", \"total_thefts\"]], on=\"HOOD_158\"\n",
    ")\n",
    "seasonal_pct[\"pct_of_hood_thefts\"] = (\n",
    "    seasonal_pct[\"seasonal_thefts\"] / seasonal_pct[\"total_thefts\"]\n",
    ") * 100\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nTop 10 Neighborhoods by Total Auto Thefts:\")\n",
    "display(theft_by_hood.sort_values(\"total_thefts\", ascending=False).head(10))\n",
    "\n",
    "# Pivot seasonal data for easier analysis\n",
    "seasonal_pivot = seasonal_pct.pivot_table(\n",
    "    index=\"HOOD_158\", columns=\"SEASON\", values=\"pct_of_hood_thefts\", fill_value=0\n",
    ")\n",
    "\n",
    "print(\"\\nSeasonal Patterns in Top 5 Neighborhoods:\")\n",
    "display(\n",
    "    seasonal_pivot.loc[\n",
    "        theft_by_hood.sort_values(\"total_thefts\", ascending=False).head(5)[\"HOOD_158\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Census Data\n",
    "\n",
    "Next, we need to prepare the census dataset for integration with our theft data. The census data contains numerous socioeconomic indicators. For this analysis, we'll focus on a subset of relevant indicators that might be associated with auto theft rates, including:\n",
    "\n",
    "- Income measures\n",
    "- Housing characteristics\n",
    "- Education levels\n",
    "- Employment statistics\n",
    "\n",
    "We'll extract and consolidate these indicators by FSA before joining them with neighborhood-level theft data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of characteristics in the census data\n",
    "print(\"Census dataset characteristics:\")\n",
    "unique_chars = census_df[\"CHARACTERISTIC_NAME\"][\n",
    "    census_df[\"CHARACTERISTIC_LEVEL\"] == 1\n",
    "].unique()\n",
    "print(f\"Total unique characteristics: {len(unique_chars)}\")\n",
    "\n",
    "# Let's see some examples of characteristics\n",
    "print(\"\\nSample of available census characteristics:\")\n",
    "display(pd.Series(unique_chars).sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of characteristics in the census data\n",
    "print(\"Census dataset characteristics:\")\n",
    "unique_chars = census_df[\"CHARACTERISTIC_NAME\"].unique()\n",
    "print(f\"Total unique characteristics: {len(unique_chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_df[census_df[\"ALT_GEO_CODE\"] == \"M9W\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the characteristics, we'll select some relevant socioeconomic indicators\n",
    "# Let's focus on income, housing, education, and demographic factors\n",
    "\n",
    "# Define characteristics of interest\n",
    "income_chars = [\n",
    "    \"Average total income of household in 2020 ($)\",\n",
    "    \"Average total income of economic family in 2020 ($)\",\n",
    "    \"Average total income in 2020 ($)\",\n",
    "]\n",
    "\n",
    "employment_chars = [\n",
    "    \"Employment rate\",\n",
    "    \"Unemployment rate\",\n",
    "    \"Participation rate\",\n",
    "]\n",
    "\n",
    "transportation_chars = [\n",
    "    \"Car, truck or van\",\n",
    "    \"Public transit\",\n",
    "]\n",
    "\n",
    "housing_chars = [\n",
    "    \"Average value of dwellings ($)\",\n",
    "    \"Average monthly shelter costs for rented dwellings ($)\",\n",
    "    \"Average monthly shelter costs for owned dwellings ($)\",\n",
    "    \"Renter\",\n",
    "    \"Owner\",\n",
    "]\n",
    "\n",
    "immigration_chars = [\n",
    "    \"Total - Generation status for the population in private households - 25% sample data\",\n",
    "    \"Immigrants\",\n",
    "    \"Non-immigrants\",\n",
    "    \"Non-permanent residents\",\n",
    "]\n",
    "\n",
    "education_chars = [\n",
    "    \"No certificate, diploma or degree\",\n",
    "    \"Postsecondary certificate, diploma or degree\",\n",
    "    \"High (secondary) school diploma or equivalency certificate\",\n",
    "]\n",
    "\n",
    "# Combine all characteristics of interest\n",
    "chars_of_interest = (\n",
    "    income_chars\n",
    "    + housing_chars\n",
    "    + education_chars\n",
    "    + employment_chars\n",
    "    + transportation_chars\n",
    "    + immigration_chars\n",
    ")\n",
    "\n",
    "# Filter the census data to only include our characteristics of interest\n",
    "selected_census = census_df[\n",
    "    census_df[\"CHARACTERISTIC_NAME\"].isin(chars_of_interest)\n",
    "].copy()\n",
    "\n",
    "# Check what we have\n",
    "print(\"\\nSelected census data characteristics:\")\n",
    "for char in chars_of_interest:\n",
    "    if char in selected_census[\"CHARACTERISTIC_NAME\"].values:\n",
    "        print(f\"- {char}: Found\")\n",
    "    else:\n",
    "        print(f\"- {char}: Not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_mapping = {\n",
    "    \"Average total income of household in 2020 ($)\": \"household_income\",\n",
    "    \"Average total income of economic family in 2020 ($)\": \"family_income\",\n",
    "    \"Average total income in 2020 ($)\": \"individual_income\",\n",
    "    \"Employment rate\": \"employment_rate\",\n",
    "    \"Unemployment rate\": \"unemployment_rate\",\n",
    "    \"Participation rate\": \"labor_participation\",\n",
    "    \"Car, truck or van\": \"commute_by_car\",\n",
    "    \"Public transit\": \"commute_by_transit\",\n",
    "    \"Average value of dwellings ($)\": \"avg_dwelling_value\",\n",
    "    \"Average monthly shelter costs for rented dwellings ($)\": \"avg_rent_cost\",\n",
    "    \"Average monthly shelter costs for owned dwellings ($)\": \"avg_mortgage_cost\",\n",
    "    \"Renter\": \"renter_count\",\n",
    "    \"Owner\": \"owner_count\",\n",
    "    \"Total - Generation status for the population in private households - 25% sample data\": \"total_population\",  # noqa: E501\n",
    "    \"Immigrants\": \"immigrant_count\",\n",
    "    \"Non-immigrants\": \"non_immigrant_count\",\n",
    "    \"Non-permanent residents\": \"non_permanent_resident_count\",\n",
    "    \"No certificate, diploma or degree\": \"no_degree\",\n",
    "    \"Postsecondary certificate, diploma or degree\": \"postsecondary_education\",\n",
    "    \"High (secondary) school diploma or equivalency certificate\": \"high_school_diploma\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data for analysis: pivot to get FSAs as rows and characteristics as columns\n",
    "# We'll use the C1_COUNT_TOTAL column which contains the actual values\n",
    "census_pivot = selected_census.pivot_table(\n",
    "    index=\"ALT_GEO_CODE\", columns=\"CHARACTERISTIC_NAME\", values=\"C1_COUNT_TOTAL\"\n",
    ")\n",
    "\n",
    "# Rename columns to make them more manageable\n",
    "# Rename columns using our defined mapping\n",
    "for old_col in census_pivot.columns:\n",
    "    if old_col in column_name_mapping:\n",
    "        census_pivot = census_pivot.rename(\n",
    "            columns={old_col: column_name_mapping[old_col]}\n",
    "        )\n",
    "    else:\n",
    "        # Fallback to the automated approach for any columns not in our mapping\n",
    "        new_col = (\n",
    "            old_col.replace(\"(\", \"\")\n",
    "            .replace(\")\", \"\")\n",
    "            .replace(\",\", \"\")\n",
    "            .replace(\" \", \"_\")\n",
    "            .replace(\"-\", \"_\")\n",
    "            .replace(\"%\", \"pct\")\n",
    "            .replace(\"'\", \"\")\n",
    "            .replace(\"$\", \"dollars\")\n",
    "            .lower()\n",
    "        )\n",
    "        census_pivot = census_pivot.rename(columns={old_col: new_col})\n",
    "\n",
    "\n",
    "# Display the pivoted data\n",
    "print(\"\\nPivoted census data by FSA:\")\n",
    "display(census_pivot)\n",
    "\n",
    "# Reset index to prepare for merging\n",
    "census_pivot = census_pivot.reset_index()\n",
    "census_pivot = census_pivot.rename(columns={\"alt_geo_code\": \"CFSAUID\"})\n",
    "\n",
    "# Now we have the census data ready to be joined with our auto theft data using FSAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Step 3: Implement Areal-Weighted Interpolation\n",
    "\n",
    "Now we'll use the spatial relationship data in `overlap_df` to distribute the census data from FSAs to neighborhoods. This process, called areal-weighted interpolation, assumes that population characteristics are uniformly distributed within each FSA.\n",
    "\n",
    "For each neighborhood-FSA pair, we'll:\n",
    "\n",
    "1. Calculate the contribution of each FSA to each neighborhood based on area overlap\n",
    "2. Apply those weights to distribute census variables from FSAs to neighborhoods\n",
    "3. Aggregate the distributed values to get estimated census characteristics for each neighborhood\n",
    "\n",
    "This allows us to transform FSA-level census data into neighborhood-level data that can be joined with our auto theft dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's rename the neighborhood code column in overlap_df to match our theft data\n",
    "overlap_df = overlap_df.rename(columns={\"AREA_LONG_CODE\": \"HOOD_158\"})\n",
    "\n",
    "# Convert hood codes to same format as in theft_df\n",
    "overlap_df[\"HOOD_158\"] = overlap_df[\"HOOD_158\"].astype(str).str.zfill(3)\n",
    "\n",
    "# Now merge the census data with overlap data\n",
    "fsa_hood_census = pd.merge(\n",
    "    overlap_df, census_pivot, how=\"left\", left_on=\"CFSAUID\", right_on=\"ALT_GEO_CODE\"\n",
    ")\n",
    "\n",
    "# Check the merged data\n",
    "print(f\"Shape of merged FSA-hood-census data: {fsa_hood_census.shape}\")\n",
    "display(fsa_hood_census)\n",
    "\n",
    "# Let's compute the weighted census values based on overlap percentages\n",
    "# We need to multiply each census value by the overlap_percent\n",
    "\n",
    "# First, identify the census columns (not CFSAUID, HOOD_158, or overlap_percent)\n",
    "census_cols = [\n",
    "    col\n",
    "    for col in fsa_hood_census.columns\n",
    "    if col not in [\"CFSAUID\", \"HOOD_158\", \"overlap_percent\", \"ALT_GEO_CODE\"]\n",
    "]\n",
    "\n",
    "# Apply the weighting by multiplying each census value by the overlap_percent\n",
    "for col in census_cols:\n",
    "    fsa_hood_census[f\"weighted_{col}\"] = (\n",
    "        fsa_hood_census[col] * fsa_hood_census[\"overlap_percent\"]\n",
    "    )\n",
    "\n",
    "# Now aggregate the weighted values by neighborhood\n",
    "weighted_cols = [f\"weighted_{col}\" for col in census_cols]\n",
    "hood_census = fsa_hood_census.groupby(\"HOOD_158\")[weighted_cols].sum().reset_index()\n",
    "\n",
    "# Rename columns back to original census column names\n",
    "for wcol, col in zip(weighted_cols, census_cols, strict=False):\n",
    "    hood_census = hood_census.rename(columns={wcol: col})\n",
    "\n",
    "# Now we have the census data aggregated at the neighborhood level\n",
    "print(\"\\nCensus data aggregated to neighborhood level:\")\n",
    "display(hood_census)\n",
    "\n",
    "# Display the number of neighborhoods with census data\n",
    "print(f\"Number of neighborhoods with interpolated census data: {len(hood_census)}\")\n",
    "print(f\"Number of neighborhoods in theft data: {len(theft_by_hood)}\")\n",
    "\n",
    "# Check for any neighborhoods in theft data that aren't in the census data\n",
    "hoods_in_theft = set(theft_by_hood[\"HOOD_158\"])\n",
    "hoods_in_census = set(hood_census[\"HOOD_158\"])\n",
    "missing_hoods = hoods_in_theft - hoods_in_census\n",
    "print(\n",
    "    f\"Number of neighborhoods in theft data but not in census data: {len(missing_hoods)}\"\n",
    ")\n",
    "if len(missing_hoods) > 0:\n",
    "    print(\"Missing neighborhood codes:\")\n",
    "    print(sorted(missing_hoods))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Step 4: Join Auto Theft and Census Data\n",
    "\n",
    "Now that we have both datasets at the neighborhood level, we can join them to analyze the relationship between socioeconomic factors and auto theft incidents. We'll merge the aggregated theft data with the interpolated census data based on neighborhood codes.\n",
    "\n",
    "This combined dataset will allow us to examine correlations and potential relationships between community characteristics and theft rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the theft data with the census data by neighborhood\n",
    "combined_df = pd.merge(theft_by_hood, hood_census, on=\"HOOD_158\", how=\"inner\")\n",
    "\n",
    "# Check the combined dataset\n",
    "print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "print(f\"Number of neighborhoods in combined data: {len(combined_df)}\")\n",
    "print(\"\\nFirst few rows of combined data:\")\n",
    "display(combined_df.head())\n",
    "\n",
    "# Add a new column for theft rate (thefts per 1000 people) to normalize by population\n",
    "combined_df[\"theft_rate_per_1000\"] = (\n",
    "    combined_df[\"avg_yearly_thefts\"] / combined_df[\"total_population\"]\n",
    ") * 1000\n",
    "\n",
    "# Sort by theft rate to see neighborhoods with highest rates\n",
    "top_theft_rates = combined_df.sort_values(\"theft_rate_per_1000\", ascending=False)[\n",
    "    [\n",
    "        \"HOOD_158\",\n",
    "        \"theft_rate_per_1000\",\n",
    "        \"total_thefts\",\n",
    "        \"avg_yearly_thefts\",\n",
    "        \"total_population\",\n",
    "    ]\n",
    "].head(10)\n",
    "\n",
    "print(\"\\nNeighborhoods with highest auto theft rates per 1000 people:\")\n",
    "display(top_theft_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Step 5: Correlation Analysis\n",
    "\n",
    "Let's analyze the relationships between auto theft rates and various socioeconomic factors using correlation analysis. This will help us identify which factors might be most strongly associated with auto theft incidents in Toronto neighborhoods.\n",
    "\n",
    "We'll examine correlations with metrics such as:\n",
    "\n",
    "- Income levels\n",
    "- Housing values and costs\n",
    "- Education levels\n",
    "- Population density\n",
    "\n",
    "Through this analysis, we can identify potential socioeconomic drivers of auto theft patterns across Toronto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations between theft metrics and census variables\n",
    "# Select relevant columns for correlation analysis\n",
    "theft_metrics = [\"total_thefts\", \"theft_rate_per_1000\"]\n",
    "\n",
    "# Select relevant census variables based on column name mapping\n",
    "income_vars = [\n",
    "    \"household_income\",\n",
    "    \"family_income\",\n",
    "    \"individual_income\",\n",
    "]\n",
    "\n",
    "housing_vars = [\n",
    "    \"avg_dwelling_value\",\n",
    "    \"avg_rent_cost\",\n",
    "    \"avg_mortgage_cost\",\n",
    "    \"renter_count\",\n",
    "    \"owner_count\",\n",
    "]\n",
    "\n",
    "education_vars = [\n",
    "    \"no_degree\",\n",
    "    \"postsecondary_education\",\n",
    "    \"high_school_diploma\",\n",
    "]\n",
    "\n",
    "employment_vars = [\n",
    "    \"employment_rate\",\n",
    "    \"unemployment_rate\",\n",
    "    \"labor_participation\",\n",
    "]\n",
    "\n",
    "transportation_vars = [\n",
    "    \"commute_by_car\",\n",
    "    \"commute_by_transit\",\n",
    "]\n",
    "\n",
    "demographic_vars = [\n",
    "    \"total_population\",\n",
    "    \"immigrant_count\",\n",
    "    \"non_immigrant_count\",\n",
    "    \"non_permanent_resident_count\",\n",
    "]\n",
    "\n",
    "# Combine all variables for correlation analysis\n",
    "corr_vars = (\n",
    "    income_vars\n",
    "    + housing_vars\n",
    "    + education_vars\n",
    "    + employment_vars\n",
    "    + transportation_vars\n",
    "    + demographic_vars\n",
    ")\n",
    "\n",
    "# Select columns that exist in the dataframe\n",
    "existing_corr_vars = [var for var in corr_vars if var in combined_df.columns]\n",
    "\n",
    "# Calculate correlations between theft metrics and census variables\n",
    "correlation_df = combined_df[theft_metrics + existing_corr_vars].corr()\n",
    "\n",
    "# Focus on correlations with theft metrics\n",
    "theft_correlations = correlation_df.loc[existing_corr_vars, theft_metrics]\n",
    "\n",
    "# Sort by absolute correlation with theft_rate_per_1000\n",
    "theft_correlations = theft_correlations.sort_values(\n",
    "    by=\"theft_rate_per_1000\", key=abs, ascending=False\n",
    ")\n",
    "\n",
    "print(\"\\nCorrelations between census variables and theft metrics:\")\n",
    "display(theft_correlations)\n",
    "\n",
    "# Visualize top correlations with theft rate\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    theft_correlations.head(10), cmap=\"coolwarm\", annot=True, fmt=\".2f\", center=0\n",
    ")\n",
    "plt.title(\"Top Socioeconomic Correlations with Auto Theft Metrics\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create scatter plots for top correlating variables with theft rate\n",
    "top_correlating_vars = theft_correlations.index[:5]  # Top 5 correlating variables\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, var in enumerate(top_correlating_vars):\n",
    "    if i < len(axes):\n",
    "        sns.scatterplot(x=var, y=\"theft_rate_per_1000\", data=combined_df, ax=axes[i])\n",
    "        axes[i].set_title(f\"Theft Rate vs {var}\", fontsize=12)\n",
    "        axes[i].set_xlabel(var, fontsize=10)\n",
    "        axes[i].set_ylabel(\"Theft Rate per 1000 People\", fontsize=10)\n",
    "\n",
    "        # Add regression line\n",
    "        sns.regplot(\n",
    "            x=var,\n",
    "            y=\"theft_rate_per_1000\",\n",
    "            data=combined_df,\n",
    "            scatter=False,\n",
    "            ax=axes[i],\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "# Handle any unused subplot\n",
    "for i in range(len(top_correlating_vars), len(axes)):\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Step 6: Key Insights and Conclusions\n",
    "\n",
    "Based on our analysis of auto theft patterns in Toronto neighborhoods and their relationship with socioeconomic factors, we can draw several insights:\n",
    "\n",
    "1. **Spatial Distribution**: Auto theft is not evenly distributed across Toronto. Certain neighborhoods consistently show higher theft rates, which may be influenced by a combination of socioeconomic factors and environmental conditions.\n",
    "\n",
    "2. **Socioeconomic Correlations**: We identified several socioeconomic factors that exhibit meaningful correlations with auto theft rates, potentially indicating underlying patterns related to opportunity, neighborhood characteristics, and community dynamics.\n",
    "\n",
    "3. **Seasonal Patterns**: Seasonal variations in auto theft incidents suggest potential relationships with environmental factors, population mobility patterns, or changes in opportunity structures throughout the year.\n",
    "\n",
    "These insights demonstrate the value of combining crime data with census information through spatial interpolation techniques. This integrated approach provides a more comprehensive understanding of auto theft patterns in Toronto and could help inform targeted prevention strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional insights from our analysis\n",
    "\n",
    "# Insight 1: Calculate theft hotspots by looking at neighborhoods with significantly higher theft rates\n",
    "mean_theft_rate = combined_df[\"theft_rate_per_1000\"].mean()\n",
    "std_theft_rate = combined_df[\"theft_rate_per_1000\"].std()\n",
    "hotspot_threshold = mean_theft_rate + std_theft_rate\n",
    "\n",
    "hotspots = combined_df[\n",
    "    combined_df[\"theft_rate_per_1000\"] > hotspot_threshold\n",
    "].sort_values(\"theft_rate_per_1000\", ascending=False)\n",
    "\n",
    "print(f\"Mean theft rate per 1000 people: {mean_theft_rate:.2f}\")\n",
    "print(f\"Standard deviation: {std_theft_rate:.2f}\")\n",
    "print(f\"Hotspot threshold (mean + 1 std): {hotspot_threshold:.2f}\")\n",
    "print(f\"\\nIdentified {len(hotspots)} neighborhood hotspots for auto theft:\")\n",
    "display(\n",
    "    hotspots[\n",
    "        [\n",
    "            \"HOOD_158\",\n",
    "            \"theft_rate_per_1000\",\n",
    "            \"total_thefts\",\n",
    "            \"avg_yearly_thefts\",\n",
    "            \"total_population\",\n",
    "            \"household_income\",  # Using mapped column names\n",
    "            \"avg_dwelling_value\",\n",
    "        ]\n",
    "    ].head(10)\n",
    ")\n",
    "\n",
    "# Insight 2: Compare high vs. low theft rate neighborhoods in terms of key socioeconomic factors\n",
    "# Define high and low theft rate neighborhoods (top 25% and bottom 25%)\n",
    "high_theft = combined_df[\n",
    "    combined_df[\"theft_rate_per_1000\"]\n",
    "    > combined_df[\"theft_rate_per_1000\"].quantile(0.75)\n",
    "]\n",
    "low_theft = combined_df[\n",
    "    combined_df[\"theft_rate_per_1000\"]\n",
    "    < combined_df[\"theft_rate_per_1000\"].quantile(0.25)\n",
    "]\n",
    "\n",
    "# Select key socioeconomic variables to compare based on our mapping\n",
    "key_vars = [\n",
    "    \"household_income\",\n",
    "    \"individual_income\",\n",
    "    \"avg_dwelling_value\",\n",
    "    \"employment_rate\",\n",
    "    \"unemployment_rate\",\n",
    "    \"commute_by_car\",\n",
    "    \"commute_by_transit\",\n",
    "    \"immigrant_count\",\n",
    "    \"postsecondary_education\",\n",
    "]\n",
    "\n",
    "# Filter to variables that exist in the dataframe\n",
    "existing_key_vars = [var for var in key_vars if var in combined_df.columns]\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"High Theft Areas\": high_theft[existing_key_vars].mean(),\n",
    "        \"Low Theft Areas\": low_theft[existing_key_vars].mean(),\n",
    "    }\n",
    ")\n",
    "comparison[\"Percent Difference\"] = (\n",
    "    (comparison[\"High Theft Areas\"] - comparison[\"Low Theft Areas\"])\n",
    "    / comparison[\"Low Theft Areas\"]\n",
    "    * 100\n",
    ")\n",
    "\n",
    "print(\"\\nInsight 2: Comparison of high vs. low theft rate neighborhoods:\")\n",
    "display(comparison)\n",
    "\n",
    "# Insight 3: Analyze transportation patterns in high theft neighborhoods\n",
    "# Look at commute methods in high vs. low theft areas\n",
    "if (\n",
    "    \"commute_by_car\" in combined_df.columns\n",
    "    and \"commute_by_transit\" in combined_df.columns\n",
    "):\n",
    "    # Create normalized comparison (percentage of total commuters)\n",
    "    high_theft[\"car_transit_ratio\"] = (\n",
    "        high_theft[\"commute_by_car\"] / high_theft[\"commute_by_transit\"]\n",
    "    )\n",
    "    low_theft[\"car_transit_ratio\"] = (\n",
    "        low_theft[\"commute_by_car\"] / low_theft[\"commute_by_transit\"]\n",
    "    )\n",
    "\n",
    "    print(\"\\nInsight 3: Transportation patterns in high vs. low theft areas:\")\n",
    "    print(\n",
    "        f\"Average car to transit ratio in high theft areas: {high_theft['car_transit_ratio'].mean():.2f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Average car to transit ratio in low theft areas: {low_theft['car_transit_ratio'].mean():.2f}\"\n",
    "    )\n",
    "\n",
    "    # Top 5 neighborhoods with highest car ownership and high theft rates\n",
    "    car_rich_theft_areas = high_theft.sort_values(\n",
    "        \"commute_by_car\", ascending=False\n",
    "    ).head(5)\n",
    "    print(\"\\nTop 5 neighborhoods with highest car usage and high theft rates:\")\n",
    "    display(\n",
    "        car_rich_theft_areas[\n",
    "            [\"HOOD_158\", \"theft_rate_per_1000\", \"commute_by_car\", \"commute_by_transit\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Insight 4: Education and employment analysis\n",
    "# Examine relationship between education levels and theft rates\n",
    "education_employment_vars = [\n",
    "    \"postsecondary_education\",\n",
    "    \"high_school_diploma\",\n",
    "    \"no_degree\",\n",
    "    \"employment_rate\",\n",
    "    \"unemployment_rate\",\n",
    "    \"labor_participation\",\n",
    "]\n",
    "\n",
    "# Filter to variables that exist in the dataframe\n",
    "existing_edu_emp_vars = [\n",
    "    var for var in education_employment_vars if var in combined_df.columns\n",
    "]\n",
    "\n",
    "if existing_edu_emp_vars:\n",
    "    # Calculate correlations with theft rate\n",
    "    edu_emp_corr = (\n",
    "        combined_df[[\"theft_rate_per_1000\"] + existing_edu_emp_vars].corr().iloc[1:, 0]\n",
    "    )\n",
    "\n",
    "    print(\"\\nInsight 4: Education and employment correlations with theft rate:\")\n",
    "    display(edu_emp_corr.sort_values(ascending=False))\n",
    "\n",
    "    # Create scatter plot of most significant relationship\n",
    "    if len(existing_edu_emp_vars) > 0:\n",
    "        top_factor = edu_emp_corr.abs().sort_values(ascending=False).index[0]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.regplot(x=top_factor, y=\"theft_rate_per_1000\", data=combined_df)\n",
    "        plt.title(f\"Relationship between {top_factor} and Auto Theft Rate\")\n",
    "        plt.xlabel(top_factor)\n",
    "        plt.ylabel(\"Auto Theft Rate per 1000 People\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Revised Key Findings and Implications\n",
    "\n",
    "Our enhanced analysis of Toronto auto theft patterns has revealed several critical insights:\n",
    "\n",
    "1. **Income and Property Value Relationships**: We've identified significant correlations between household income, property values, and auto theft rates. The data suggests that neighborhoods with certain income profiles may experience different theft patterns, which has implications for targeted prevention strategies.\n",
    "\n",
    "2. **Transportation Mode Impact**: The analysis reveals distinct patterns in how transportation choices (car vs. public transit usage) relate to theft rates. Areas with higher car ownership density show different vulnerability patterns, suggesting that vehicle availability is a key factor in theft opportunity.\n",
    "\n",
    "3. **Education-Employment Connection**: There appears to be a meaningful relationship between educational attainment, employment metrics, and theft incidence. This suggests that socioeconomic stability factors play an important role in neighborhood security profiles.\n",
    "\n",
    "4. **Immigration and Demographic Patterns**: The relationship between immigrant population concentrations and theft rates provides insight into how neighborhood demographics might influence or be correlated with crime patterns.\n",
    "\n",
    "These findings point to the multifaceted nature of auto theft in Toronto, where various socioeconomic factors - including income, transportation choices, employment stability, and demographic composition - interact in complex ways. This understanding can support more nuanced approaches to crime prevention that consider the unique profile of each neighborhood rather than applying one-size-fits-all strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Future Considerations\n",
    "\n",
    "While this analysis provides valuable insights into the relationship between auto theft and socioeconomic factors in Toronto neighborhoods, several avenues for future investigation remain:\n",
    "\n",
    "1. **Improved Spatial Interpolation**: The areal-weighted interpolation method assumes uniform population distribution within FSAs. Future work could incorporate dasymetric mapping techniques using ancillary data like land use and building footprints to refine the distribution of census variables.\n",
    "\n",
    "2. **Temporal Analysis**: Examining how the relationship between auto theft and socioeconomic factors changes over time could reveal evolving patterns and trends.\n",
    "\n",
    "3. **Multivariate Modeling**: Developing predictive models that account for multiple socioeconomic factors simultaneously could provide more robust insights into the drivers of auto theft.\n",
    "\n",
    "4. **Policy Recommendations**: Translating these findings into actionable recommendations for law enforcement, urban planning, and community development initiatives.\n",
    "\n",
    "By addressing these considerations, future analyses could build upon this work to develop more nuanced insights and effective strategies for addressing auto theft in Toronto neighborhoods.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
